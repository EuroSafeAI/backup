<!DOCTYPE html><!--VDhjZ7CsAexTcJVDmJNzf--><html lang="en" class="scroll-smooth"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" as="image" href="/images/logo.png"/><link rel="stylesheet" href="/_next/static/chunks/9d6f999f02f7dc8d.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/75b31e65f0a4fc66.js"/><script src="/_next/static/chunks/75db382961acc07c.js" async=""></script><script src="/_next/static/chunks/171cd0ba1612502a.js" async=""></script><script src="/_next/static/chunks/1c9669fd87998ca9.js" async=""></script><script src="/_next/static/chunks/turbopack-486fba69726fc5c8.js" async=""></script><script src="/_next/static/chunks/744355e03808d4c7.js" async=""></script><script src="/_next/static/chunks/ff1a16fafef87110.js" async=""></script><script src="/_next/static/chunks/d2be314c3ece3fbe.js" async=""></script><script src="/_next/static/chunks/e4113e168c54d001.js" async=""></script><script src="/_next/static/chunks/bb731efc33baa73a.js" async=""></script><script src="/_next/static/chunks/00e9ab075e5969cf.js" async=""></script><link rel="preload" href="https://fonts.googleapis.com/css2?family=Libre+Baskerville:ital,wght@0,400;0,700;1,400&amp;family=IBM+Plex+Sans:ital,wght@0,300;0,400;0,500;0,600;0,700;1,400&amp;display=swap" as="style"/><link rel="preconnect" href="https://fonts.googleapis.com"/><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"/><title>Multi-Agent Safety | EuroSafeAI</title><meta name="description" content="Research on testing LLM cooperation and safety in multi-agent simulation settings, including game-theoretic benchmarking and social dilemma experiments."/><meta name="author" content="EuroSafeAI"/><meta name="keywords" content="AI Safety,AI Security,Multi-Agent Safety,Research,Nonprofit,Switzerland"/><meta property="og:title" content="EuroSafeAI - Developing Multi-Agent AI Safety for Democracy"/><meta property="og:description" content="A nonprofit research organization led by Prof. Zhijing Jin, aiming to advance AI safety and security through rigorous research, threat assessment, and mitigation strategies."/><meta property="og:locale" content="en_US"/><meta property="og:type" content="website"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:title" content="EuroSafeAI - Developing Multi-Agent AI Safety for Democracy"/><meta name="twitter:description" content="A nonprofit research organization led by Prof. Zhijing Jin, aiming to advance AI safety and security through rigorous research, threat assessment, and mitigation strategies."/><link rel="icon" href="/favicon.ico" sizes="any"/><link rel="apple-touch-icon" href="/favicon.ico"/><link href="https://fonts.googleapis.com/css2?family=Libre+Baskerville:ital,wght@0,400;0,700;1,400&amp;family=IBM+Plex+Sans:ital,wght@0,300;0,400;0,500;0,600;0,700;1,400&amp;display=swap" rel="stylesheet"/><script src="/_next/static/chunks/a6dad97d9634a72d.js" noModule=""></script></head><body class="antialiased font-jost"><div hidden=""><!--$--><!--/$--></div><div class="min-h-screen flex flex-col bg-white"><header class="w-full bg-primary-600 border-b border-primary-700 sticky top-0 z-50 transition-shadow duration-300 "><nav class="max-w-6xl mx-auto px-6 py-4 flex justify-between items-center"><a class="flex items-center gap-3" href="/"><img alt="EuroSafeAI" width="120" height="50" decoding="async" data-nimg="1" class="h-8 w-auto object-contain" style="color:transparent" src="/images/logo.png"/><span class="text-xl font-bold text-white tracking-tight">EuroSafeAI</span></a><div class="hidden md:flex items-center gap-8"><a class="text-base font-medium text-primary-100 hover:text-white transition-colors duration-150" href="/">Home</a><a class="text-base font-medium text-primary-100 hover:text-white transition-colors duration-150" href="/research">Research</a><a class="text-base font-medium text-primary-100 hover:text-white transition-colors duration-150" href="/team">Team</a><a class="text-base font-medium text-primary-100 hover:text-white transition-colors duration-150" href="/careers">Careers</a><a href="mailto:eurosafeai.zurich@gmail.com" class="px-4 py-2 bg-white text-primary-600 text-base font-semibold rounded-lg hover:bg-primary-50 transition-colors duration-150">Contact Us</a></div><button class="md:hidden p-2" aria-label="Toggle menu" aria-expanded="false"><svg class="w-6 h-6 text-white" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 6h16M4 12h16M4 18h16"></path></svg></button></nav></header><main class="flex-1"><section class="from-primary-50 via-white to-slate-50 py-16 lg:py-24"><div class="max-w-6xl mx-auto px-6 text-center"><p class="text-xs font-semibold text-primary-600 uppercase tracking-widest mb-4 font-jost motion-safe:animate-fade-in" style="animation-delay:0ms">Research Line</p><h1 class="text-4xl md:text-5xl font-extrabold text-gray-900 mb-4 motion-safe:animate-fade-slide-up" style="animation-delay:60ms">Multi-Agent AI Safety</h1><p class="text-lg md:text-xl text-gray-700 max-w-3xl mx-auto mb-4 font-jost motion-safe:animate-fade-slide-up" style="animation-delay:180ms">Testing LLM cooperation and safety in multi-agent simulation settings — from game-theoretic benchmarks to society-scale social dilemmas.</p><p class="text-gray-600 max-w-2xl mx-auto font-jost motion-safe:animate-fade-slide-up" style="animation-delay:300ms">As AI agents increasingly interact with each other, the real world, and humans, single-agent safety evaluations are no longer sufficient. We study emergent risks in collective action problems, zero-sum competitions, and public goods games.</p><div class="mt-8 flex flex-wrap justify-center gap-4 motion-safe:animate-fade-slide-up" style="animation-delay:420ms"><a class="inline-flex items-center px-6 py-3 bg-primary-700 text-white font-medium rounded-lg hover:bg-primary-800 transition-colors active:scale-95" href="/research">View All Research<svg class="w-4 h-4 ml-2 flex-shrink-0" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7"></path></svg></a></div></div></section><section class="bg-gray-50 py-16 lg:py-20"><div class="max-w-6xl mx-auto px-6"><div style="opacity:0;transform:translateY(24px)"><div class="flex items-center gap-3 mb-3"><div class="w-10 h-10 rounded-lg bg-primary-700 flex items-center justify-center flex-shrink-0"><svg class="w-5 h-5 text-white" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M11.049 2.927c.3-.921 1.603-.921 1.902 0l1.519 4.674a1 1 0 00.95.69h4.915c.969 0 1.371 1.24.588 1.81l-3.976 2.888a1 1 0 00-.363 1.118l1.518 4.674c.3.922-.755 1.688-1.538 1.118l-3.976-2.888a1 1 0 00-1.176 0l-3.976 2.888c-.783.57-1.838-.197-1.538-1.118l1.518-4.674a1 1 0 00-.363-1.118l-3.976-2.888c-.784-.57-.38-1.81.588-1.81h4.914a1 1 0 00.951-.69l1.519-4.674z"></path></svg></div><h2 class="text-2xl font-bold text-gray-900">Research</h2></div><p class="text-gray-600 mb-8 max-w-2xl">Published work and ongoing research agenda on testing cooperation in multi-agent LLM systems.</p></div><div style="opacity:0;transform:translateY(24px)"><div class="rounded-xl bg-white p-4 sm:p-6 border border-gray-100"><div class="grid grid-cols-1 sm:grid-cols-2 lg:grid-cols-3 gap-6"><article class="group rounded-lg border border-gray-200 p-5 hover:shadow-sm transition-all hover:-translate-y-0.5 bg-gray-50 flex flex-col"><div class="text-xs uppercase tracking-wide text-gray-500 mb-2">Preprint 2026</div><h3 class="text-base font-semibold text-gray-900 group-hover:underline underline-offset-4 leading-snug mb-2"><a href="https://arxiv.org/abs/2602.12316" target="_blank" rel="noopener noreferrer">GT-HarmBench: Benchmarking AI Safety Risks Through the Lens of Game Theory</a></h3><p class="text-sm text-gray-600 line-clamp-3 flex-1">When AI agents interact in high-stakes settings, do they cooperate or defect? GT-HarmBench stress-tests 15 frontier LLMs across 2,009 scenarios drawn from the MIT AI Risk Repository, structured around classic game-theoretic dilemmas—Prisoner&#x27;s Dilemma, Stag Hunt, and Chicken. Models reach socially optimal outcomes in only 62% of cases, with cooperation collapsing to 44% in pure Prisoner&#x27;s Dilemma settings. We uncover a &quot;game theory anchoring effect&quot;: explicitly framing a situation in game-theoretic terms nudges models toward selfish Nash strategies, hurting social welfare. Mechanism design interventions—mediation, contracts, and structured communication—recover 14–18% of lost welfare, pointing toward concrete paths for safer multi-agent AI deployment.</p><p class="mt-3 text-xs text-gray-500">Pepijn Cobben*, Xuanqiang Angelo Huang*, Thao Amelia Pham*, Isabel Dahlgren*, Terry Jingchen Zhang, Zhijing Jin</p><div class="mt-3 flex flex-wrap gap-2"><a href="https://arxiv.org/abs/2602.12316" target="_blank" rel="noopener noreferrer" class="inline-flex text-xs items-center gap-1 rounded border border-primary-300 text-primary-700 px-2 py-1 hover:bg-primary-50 transition-colors">Read paper <span aria-hidden="true">↗</span></a></div><div class="mt-3 flex flex-wrap gap-1.5"><span class="text-xs px-2 py-0.5 rounded-full bg-primary-100 text-primary-700">multi-agent safety</span><span class="text-xs px-2 py-0.5 rounded-full bg-primary-100 text-primary-700">game theory</span><span class="text-xs px-2 py-0.5 rounded-full bg-primary-100 text-primary-700">benchmarking</span><span class="text-xs px-2 py-0.5 rounded-full bg-primary-100 text-primary-700">LLM cooperation</span><span class="text-xs px-2 py-0.5 rounded-full bg-primary-100 text-primary-700">mechanism design</span></div></article><article class="group rounded-lg border border-gray-200 p-5 hover:shadow-sm transition-all hover:-translate-y-0.5 bg-gray-50 flex flex-col"><div class="text-xs uppercase tracking-wide text-gray-500 mb-2">NeurIPS 2024</div><h3 class="text-base font-semibold text-gray-900 group-hover:underline underline-offset-4 leading-snug mb-2"><a href="https://arxiv.org/abs/2404.16698" target="_blank" rel="noopener noreferrer">Cooperate or Collapse: Emergence of Sustainable Cooperation in a Society of LLM Agents</a></h3><p class="text-sm text-gray-600 line-clamp-3 flex-1">We introduce GovSim, a generative simulation platform to study strategic interactions and cooperative decision-making in LLMs facing a Tragedy of the Commons. Agents play as villagers sharing a finite resource across monthly rounds of acting, discussing, and reflecting. Most models fail to achieve sustainable equilibrium (&lt; 54% survival rate); agents leveraging moral reasoning achieve significantly better sustainability.</p><p class="mt-3 text-xs text-gray-500">Giorgio Piatti*, Zhijing Jin*, Max Kleiman-Weiner*, Bernhard Schölkopf, Mrinmaya Sachan, Rada Mihalcea</p><div class="mt-3 flex flex-wrap gap-2"><a href="https://arxiv.org/abs/2404.16698" target="_blank" rel="noopener noreferrer" class="inline-flex text-xs items-center gap-1 rounded border border-primary-300 text-primary-700 px-2 py-1 hover:bg-primary-50 transition-colors">Read paper <span aria-hidden="true">↗</span></a><a class="inline-flex text-xs items-center gap-1 rounded border border-gray-200 text-gray-600 px-2 py-1 hover:bg-white transition-colors" href="/blog/cooperate-or-collapse">Blog post</a></div><div class="mt-3 flex flex-wrap gap-1.5"><span class="text-xs px-2 py-0.5 rounded-full bg-primary-100 text-primary-700">multi-agent LLMs</span><span class="text-xs px-2 py-0.5 rounded-full bg-primary-100 text-primary-700">social dilemma</span><span class="text-xs px-2 py-0.5 rounded-full bg-primary-100 text-primary-700">cooperation</span><span class="text-xs px-2 py-0.5 rounded-full bg-primary-100 text-primary-700">tragedy of the commons</span><span class="text-xs px-2 py-0.5 rounded-full bg-primary-100 text-primary-700">GovSim</span></div></article><article class="group rounded-lg border border-gray-200 p-5 hover:shadow-sm transition-all hover:-translate-y-0.5 bg-gray-50 flex flex-col"><div class="text-xs uppercase tracking-wide text-gray-500 mb-2">ICLR 2026</div><h3 class="text-base font-semibold text-gray-900 group-hover:underline underline-offset-4 leading-snug mb-2"><a href="https://openreview.net/forum?id=XeZ5WBIRvz" target="_blank" rel="noopener noreferrer">When Ethics and Payoffs Diverge: LLM Agents in Morally Charged Social Dilemmas</a></h3><p class="text-sm text-gray-600 line-clamp-3 flex-1">We introduce MoralSim, a framework that tests how large language models navigate situations where ethical principles conflict with financial incentives. Using prisoner&#x27;s dilemma and public goods games with moral contexts, we evaluated nine frontier models and find that no model exhibits consistently moral behavior. Game structure, moral framing, survival risk, and opponent behavior all significantly influence LLM decision-making.</p><p class="mt-3 text-xs text-gray-500">Steffen Backmann, David Guzman Piedrahita, Emanuel Tewolde, Rada Mihalcea, Bernhard Schölkopf, Zhijing Jin</p><div class="mt-3 flex flex-wrap gap-2"><a href="https://openreview.net/forum?id=XeZ5WBIRvz" target="_blank" rel="noopener noreferrer" class="inline-flex text-xs items-center gap-1 rounded border border-primary-300 text-primary-700 px-2 py-1 hover:bg-primary-50 transition-colors">Read paper <span aria-hidden="true">↗</span></a></div><div class="mt-3 flex flex-wrap gap-1.5"><span class="text-xs px-2 py-0.5 rounded-full bg-primary-100 text-primary-700">moral reasoning</span><span class="text-xs px-2 py-0.5 rounded-full bg-primary-100 text-primary-700">social dilemmas</span><span class="text-xs px-2 py-0.5 rounded-full bg-primary-100 text-primary-700">multi-agent</span><span class="text-xs px-2 py-0.5 rounded-full bg-primary-100 text-primary-700">payoff tradeoff</span><span class="text-xs px-2 py-0.5 rounded-full bg-primary-100 text-primary-700">AI ethics</span></div></article><article class="group rounded-lg border border-gray-200 p-5 hover:shadow-sm transition-all hover:-translate-y-0.5 bg-gray-50 flex flex-col"><div class="text-xs uppercase tracking-wide text-gray-500 mb-2">COLM 2025</div><h3 class="text-base font-semibold text-gray-900 group-hover:underline underline-offset-4 leading-snug mb-2"><a href="https://arxiv.org/abs/2506.23276" target="_blank" rel="noopener noreferrer">Corrupted by Reasoning: Reasoning Language Models Become Free-Riders in Public Goods Games</a></h3><p class="text-sm text-gray-600 line-clamp-3 flex-1">We examine how language models handle cooperation in multi-agent systems by adapting a public goods game framework. We find that advanced reasoning models like o1 paradoxically underperform at maintaining cooperation compared to traditional LLMs, suggesting that the current approach to improving LLMs—focusing on reasoning capabilities—does not necessarily lead to cooperation. This has important implications for deploying autonomous AI agents in collaborative environments.</p><p class="mt-3 text-xs text-gray-500">David Guzman Piedrahita, Yongjin Yang, Mrinmaya Sachan, Giorgia Ramponi, Bernhard Schölkopf, Zhijing Jin</p><div class="mt-3 flex flex-wrap gap-2"><a href="https://arxiv.org/abs/2506.23276" target="_blank" rel="noopener noreferrer" class="inline-flex text-xs items-center gap-1 rounded border border-primary-300 text-primary-700 px-2 py-1 hover:bg-primary-50 transition-colors">Read paper <span aria-hidden="true">↗</span></a></div><div class="mt-3 flex flex-wrap gap-1.5"><span class="text-xs px-2 py-0.5 rounded-full bg-primary-100 text-primary-700">sanctioning</span><span class="text-xs px-2 py-0.5 rounded-full bg-primary-100 text-primary-700">public goods</span><span class="text-xs px-2 py-0.5 rounded-full bg-primary-100 text-primary-700">reasoning models</span><span class="text-xs px-2 py-0.5 rounded-full bg-primary-100 text-primary-700">cooperation</span><span class="text-xs px-2 py-0.5 rounded-full bg-primary-100 text-primary-700">free-rider problem</span></div></article><article class="group rounded-lg border border-gray-200 p-5 hover:shadow-sm transition-all hover:-translate-y-0.5 bg-gray-50 flex flex-col"><div class="text-xs uppercase tracking-wide text-gray-500 mb-2">Preprint 2025</div><h3 class="text-base font-semibold text-gray-900 group-hover:underline underline-offset-4 leading-snug mb-2"><a href="https://arxiv.org/abs/2506.22957" target="_blank" rel="noopener noreferrer">Agent-to-Agent Theory of Mind: Testing Interlocutor Awareness among Large Language Models</a></h3><p class="text-sm text-gray-600 line-clamp-3 flex-1">We investigate how LLMs recognize and adapt to their conversation partners&#x27; characteristics, introducing &quot;interlocutor awareness&quot;—an LLM&#x27;s capacity to identify dialogue partner traits across reasoning patterns, linguistic style, and alignment preferences. LLMs can reliably identify same-family peers and prominent model families like GPT and Claude. This capability enables enhanced multi-agent collaboration but also introduces new vulnerabilities including reward-hacking behaviors and increased jailbreak susceptibility.</p><p class="mt-3 text-xs text-gray-500">Younwoo Choi, Changling Li, Yongjin Yang, Zhijing Jin</p><div class="mt-3 flex flex-wrap gap-2"><a href="https://arxiv.org/abs/2506.22957" target="_blank" rel="noopener noreferrer" class="inline-flex text-xs items-center gap-1 rounded border border-primary-300 text-primary-700 px-2 py-1 hover:bg-primary-50 transition-colors">Read paper <span aria-hidden="true">↗</span></a></div><div class="mt-3 flex flex-wrap gap-1.5"><span class="text-xs px-2 py-0.5 rounded-full bg-primary-100 text-primary-700">theory of mind</span><span class="text-xs px-2 py-0.5 rounded-full bg-primary-100 text-primary-700">interlocutor awareness</span><span class="text-xs px-2 py-0.5 rounded-full bg-primary-100 text-primary-700">multi-agent</span><span class="text-xs px-2 py-0.5 rounded-full bg-primary-100 text-primary-700">adaptation</span><span class="text-xs px-2 py-0.5 rounded-full bg-primary-100 text-primary-700">jailbreak</span></div></article><article class="group rounded-lg border border-gray-200 p-5 hover:shadow-sm transition-all hover:-translate-y-0.5 bg-gray-50 flex flex-col"><div class="inline-flex items-center gap-1 rounded border border-amber-400/50 text-amber-700 px-2 py-0.5 text-[11px] self-start mb-2">Coming soon</div><h3 class="text-base font-semibold text-gray-900 group-hover:underline underline-offset-4 leading-snug mb-2">GovSim-Elect / AgentElect</h3><p class="text-sm text-gray-600 line-clamp-3 flex-1">A simulation of elections in multi-agent LLM societies. Examining how AI agents vote, campaign, and coordinate under democratic voting systems—and what incentives shape their electoral behavior.</p><div class="mt-3 flex flex-wrap gap-2"><span class="inline-flex items-center gap-1 rounded border border-red-400/40 text-red-600 px-2 py-1 text-[11px]">Paper coming soon</span></div><div class="mt-3 flex flex-wrap gap-1.5"><span class="text-xs px-2 py-0.5 rounded-full bg-primary-100 text-primary-700">elections</span><span class="text-xs px-2 py-0.5 rounded-full bg-primary-100 text-primary-700">multi-agent LLMs</span><span class="text-xs px-2 py-0.5 rounded-full bg-primary-100 text-primary-700">governance</span><span class="text-xs px-2 py-0.5 rounded-full bg-primary-100 text-primary-700">simulation</span><span class="text-xs px-2 py-0.5 rounded-full bg-primary-100 text-primary-700">democracy</span></div></article><article class="group rounded-lg border border-gray-200 p-5 hover:shadow-sm transition-all hover:-translate-y-0.5 bg-gray-50 flex flex-col"><div class="inline-flex items-center gap-1 rounded border border-amber-400/50 text-amber-700 px-2 py-0.5 text-[11px] self-start mb-2">Coming soon</div><h3 class="text-base font-semibold text-gray-900 group-hover:underline underline-offset-4 leading-snug mb-2">CoopEval</h3><p class="text-sm text-gray-600 line-clamp-3 flex-1">Benchmarking cooperation-sustaining mechanisms and LLM agents in social dilemmas. Translating game-theoretic mechanisms to real evaluation settings to identify what makes cooperation robust at scale.</p><div class="mt-3 flex flex-wrap gap-2"><span class="inline-flex items-center gap-1 rounded border border-red-400/40 text-red-600 px-2 py-1 text-[11px]">Paper coming soon</span></div><div class="mt-3 flex flex-wrap gap-1.5"><span class="text-xs px-2 py-0.5 rounded-full bg-primary-100 text-primary-700">cooperation</span><span class="text-xs px-2 py-0.5 rounded-full bg-primary-100 text-primary-700">benchmarking</span><span class="text-xs px-2 py-0.5 rounded-full bg-primary-100 text-primary-700">social dilemmas</span><span class="text-xs px-2 py-0.5 rounded-full bg-primary-100 text-primary-700">mechanism design</span></div></article></div></div></div></div></section><section class="bg-gray-50 py-16 lg:py-20"><div class="max-w-6xl mx-auto px-6"><div style="opacity:0;transform:translateY(24px)"><div class="flex items-center gap-3 mb-8"><div class="w-10 h-10 rounded-lg bg-primary-100 flex items-center justify-center flex-shrink-0"><svg class="w-5 h-5 text-primary-600" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M3 8l7.89 5.26a2 2 0 002.22 0L21 8M5 19h14a2 2 0 002-2V7a2 2 0 00-2-2H5a2 2 0 00-2 2v10a2 2 0 002 2z"></path></svg></div><h2 class="text-2xl font-bold text-gray-900">Media Contact</h2></div></div><div style="opacity:0;transform:translateY(24px)"><div class="grid sm:grid-cols-2 gap-6 mb-8"><div class="bg-white rounded-xl border border-gray-100 shadow-sm p-6"><div class="flex items-center gap-4"><a href="https://zhijing-jin.com/home/" target="_blank" rel="noopener noreferrer" class="shrink-0"><img alt="Zhijing Jin" loading="lazy" width="48" height="48" decoding="async" data-nimg="1" class="w-12 h-12 rounded-full object-cover ring-1 ring-gray-200" style="color:transparent" src="/images/team/zhijing-jin.png"/></a><div><a href="https://zhijing-jin.com/home/" target="_blank" rel="noopener noreferrer" class="font-semibold text-gray-900 hover:text-primary-600 transition-colors">Zhijing Jin</a><p class="text-sm text-gray-500">Founder &amp; Head, Jinesis AI Lab</p><a href="mailto:zjin.admin@cs.toronto.edu" class="text-sm text-primary-600 hover:underline">zjin.admin@cs.toronto.edu</a></div></div></div><div class="bg-white rounded-xl border border-gray-100 shadow-sm p-6"><div class="flex items-center gap-4"><a href="https://vesaterra.github.io/" target="_blank" rel="noopener noreferrer" class="shrink-0"><img alt="Punya Syon Pandey" loading="lazy" width="48" height="48" decoding="async" data-nimg="1" class="w-12 h-12 rounded-full object-cover ring-1 ring-gray-200" style="color:transparent" src="/images/team/punya-pandey.png"/></a><div><a href="https://vesaterra.github.io/" target="_blank" rel="noopener noreferrer" class="font-semibold text-gray-900 hover:text-primary-600 transition-colors">Punya Syon Pandey</a><p class="text-sm text-gray-500">Lab Assistant</p><a href="mailto:ppandey@cs.toronto.edu" class="text-sm text-primary-600 hover:underline">ppandey@cs.toronto.edu</a></div></div></div></div></div><div style="opacity:0;transform:translateY(24px)"><div class="flex flex-wrap items-center gap-4 text-sm"><a href="https://bsky.app/profile/zhijingjin.bsky.social" target="_blank" rel="noopener noreferrer" class="inline-flex items-center gap-2 px-4 py-2 rounded-lg border border-gray-200 hover:border-primary-200 hover:bg-primary-50 transition-colors">Bluesky</a><a href="https://x.com/ZhijingJin" target="_blank" rel="noopener noreferrer" class="inline-flex items-center gap-2 px-4 py-2 rounded-lg border border-gray-200 hover:border-primary-200 hover:bg-primary-50 transition-colors">X / Twitter</a><a href="https://youtube.com/@Zhijing" target="_blank" rel="noopener noreferrer" class="inline-flex items-center gap-2 px-4 py-2 rounded-lg border border-gray-200 hover:border-primary-200 hover:bg-primary-50 transition-colors">YouTube</a></div></div></div></section><div style="opacity:0;transform:translateY(24px)"><section class="bg-primary-700 py-16"><div class="max-w-4xl mx-auto px-6 text-center"><h2 class="text-3xl font-bold text-white mb-4">Explore Our Research</h2><p class="text-lg text-primary-100 mb-8 max-w-2xl mx-auto font-jost">View all our publications across AI safety, multi-agent systems, and democracy defense.</p><div class="flex flex-wrap justify-center gap-4"><a class="inline-flex items-center px-6 py-3 bg-white text-primary-700 font-medium rounded-lg hover:bg-primary-50 transition-colors active:scale-95" href="/research">All Research<svg class="w-4 h-4 ml-2 flex-shrink-0" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7"></path></svg></a><a href="mailto:eurosafeai.zurich@gmail.com" class="inline-flex items-center px-6 py-3 border-2 border-white text-white font-medium rounded-lg hover:bg-white/10 transition-colors">Contact Us</a></div></div></section></div></main><footer class="bg-gray-900 text-white"><div class="max-w-6xl mx-auto px-6 py-12"><div class="grid md:grid-cols-4 gap-8"><div class="md:col-span-2"><img alt="EuroSafeAI" loading="lazy" width="120" height="50" decoding="async" data-nimg="1" class="h-8 w-auto object-contain brightness-0 invert mb-4" style="color:transparent" src="/images/logo.png"/><p class="text-gray-400 text-sm leading-relaxed mb-4 max-w-sm">EuroSafeAI is a nonprofit research organization registered under Swiss law, dedicated to advancing AI safety and security. We have a deep collaboration with the University of Toronto.</p></div><div><h4 class="font-semibold text-white mb-4">Quick Links</h4><ul class="space-y-2"><li><a class="text-gray-400 hover:text-white text-sm transition-colors" href="/">Home</a></li><li><a class="text-gray-400 hover:text-white text-sm transition-colors" href="/research">Research</a></li><li><a class="text-gray-400 hover:text-white text-sm transition-colors" href="/team">Our Team</a></li><li><a class="text-gray-400 hover:text-white text-sm transition-colors" href="/careers">Careers</a></li></ul></div><div><h4 class="font-semibold text-white mb-4">Contact</h4><ul class="space-y-2"><li><a href="mailto:eurosafeai.zurich@gmail.com" class="text-gray-400 hover:text-white text-sm transition-colors flex items-center gap-2"><svg class="w-4 h-4 flex-shrink-0" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M3 8l7.89 5.26a2 2 0 002.22 0L21 8M5 19h14a2 2 0 002-2V7a2 2 0 00-2-2H5a2 2 0 00-2 2v10a2 2 0 002 2z"></path></svg><span>eurosafeai.zurich@gmail.com</span></a></li><li class="text-gray-400 text-sm flex items-center gap-2"><svg class="w-4 h-4 flex-shrink-0" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M17.657 16.657L13.414 20.9a1.998 1.998 0 01-2.827 0l-4.244-4.243a8 8 0 1111.314 0z"></path><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M15 11a3 3 0 11-6 0 3 3 0 016 0z"></path></svg><span>Zurich, Switzerland</span></li></ul></div></div></div><div class="border-t border-gray-800"><div class="max-w-6xl mx-auto px-6 py-4 flex flex-col md:flex-row justify-between items-center gap-2"><p class="text-gray-500 text-sm">© <!-- -->2026<!-- --> EuroSafeAI. All rights reserved.</p><p class="text-gray-500 text-sm">Swiss nonprofit registered under Swiss Law</p></div></div></footer></div><!--$--><!--/$--><script src="/_next/static/chunks/75b31e65f0a4fc66.js" id="_R_" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[79520,[\"/_next/static/chunks/744355e03808d4c7.js\"],\"\"]\n3:I[39756,[\"/_next/static/chunks/ff1a16fafef87110.js\",\"/_next/static/chunks/d2be314c3ece3fbe.js\"],\"default\"]\n4:I[37457,[\"/_next/static/chunks/ff1a16fafef87110.js\",\"/_next/static/chunks/d2be314c3ece3fbe.js\"],\"default\"]\n5:I[70119,[\"/_next/static/chunks/744355e03808d4c7.js\",\"/_next/static/chunks/e4113e168c54d001.js\",\"/_next/static/chunks/bb731efc33baa73a.js\",\"/_next/static/chunks/00e9ab075e5969cf.js\"],\"default\"]\n11:I[68027,[\"/_next/static/chunks/ff1a16fafef87110.js\",\"/_next/static/chunks/d2be314c3ece3fbe.js\"],\"default\"]\n:HL[\"/_next/static/chunks/9d6f999f02f7dc8d.css\",\"style\"]\n:HL[\"https://fonts.googleapis.com/css2?family=Libre+Baskerville:ital,wght@0,400;0,700;1,400\u0026family=IBM+Plex+Sans:ital,wght@0,300;0,400;0,500;0,600;0,700;1,400\u0026display=swap\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"VDhjZ7CsAexTcJVDmJNzf\",\"c\":[\"\",\"multi-agent-safety\"],\"q\":\"\",\"i\":false,\"f\":[[[\"\",{\"children\":[\"multi-agent-safety\",{\"children\":[\"__PAGE__\",{}]}]},\"$undefined\",\"$undefined\",true],[[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/chunks/9d6f999f02f7dc8d.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}],[\"$\",\"script\",\"script-0\",{\"src\":\"/_next/static/chunks/744355e03808d4c7.js\",\"async\":true,\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"className\":\"scroll-smooth\",\"children\":[[\"$\",\"head\",null,{\"children\":[[\"$\",\"$L2\",null,{\"id\":\"google-analytics\",\"strategy\":\"afterInteractive\",\"children\":\"\\n            (function() {\\n              var host = window.location.hostname;\\n              var id = host.endsWith('.ca') ? 'G-6RH7QF9GNV' : 'G-C4SNM3JE25';\\n              var s = document.createElement('script');\\n              s.async = true;\\n              s.src = 'https://www.googletagmanager.com/gtag/js?id=' + id;\\n              document.head.appendChild(s);\\n              window.dataLayer = window.dataLayer || [];\\n              function gtag(){dataLayer.push(arguments);}\\n              gtag('js', new Date());\\n              gtag('config', id);\\n            })();\\n          \"}],[\"$\",\"link\",null,{\"rel\":\"preconnect\",\"href\":\"https://fonts.googleapis.com\"}],[\"$\",\"link\",null,{\"rel\":\"preconnect\",\"href\":\"https://fonts.gstatic.com\",\"crossOrigin\":\"anonymous\"}],[\"$\",\"link\",null,{\"href\":\"https://fonts.googleapis.com/css2?family=Libre+Baskerville:ital,wght@0,400;0,700;1,400\u0026family=IBM+Plex+Sans:ital,wght@0,300;0,400;0,500;0,600;0,700;1,400\u0026display=swap\",\"rel\":\"stylesheet\"}]]}],[\"$\",\"body\",null,{\"className\":\"antialiased font-jost\",\"children\":[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}]]}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[[\"$\",\"div\",null,{\"className\":\"min-h-screen flex flex-col bg-white\",\"children\":[[\"$\",\"$L5\",null,{}],[\"$\",\"main\",null,{\"className\":\"flex-1\",\"children\":[[\"$\",\"section\",null,{\"className\":\"from-primary-50 via-white to-slate-50 py-16 lg:py-24\",\"children\":[\"$\",\"div\",null,{\"className\":\"max-w-6xl mx-auto px-6 text-center\",\"children\":[[\"$\",\"p\",null,{\"className\":\"text-xs font-semibold text-primary-600 uppercase tracking-widest mb-4 font-jost motion-safe:animate-fade-in\",\"style\":{\"animationDelay\":\"0ms\"},\"children\":\"Research Line\"}],[\"$\",\"h1\",null,{\"className\":\"text-4xl md:text-5xl font-extrabold text-gray-900 mb-4 motion-safe:animate-fade-slide-up\",\"style\":{\"animationDelay\":\"60ms\"},\"children\":\"Multi-Agent AI Safety\"}],[\"$\",\"p\",null,{\"className\":\"text-lg md:text-xl text-gray-700 max-w-3xl mx-auto mb-4 font-jost motion-safe:animate-fade-slide-up\",\"style\":{\"animationDelay\":\"180ms\"},\"children\":\"Testing LLM cooperation and safety in multi-agent simulation settings — from game-theoretic benchmarks to society-scale social dilemmas.\"}],\"$L6\",\"$L7\"]}]}],\"$L8\",\"$L9\",\"$La\"]}],\"$Lb\"]}],[\"$Lc\",\"$Ld\",\"$Le\"],\"$Lf\"]}],{},null,false,false]},null,false,false]},null,false,false],\"$L10\",false]],\"m\":\"$undefined\",\"G\":[\"$11\",[]],\"S\":true}\n"])</script><script>self.__next_f.push([1,"12:I[22016,[\"/_next/static/chunks/744355e03808d4c7.js\",\"/_next/static/chunks/e4113e168c54d001.js\",\"/_next/static/chunks/bb731efc33baa73a.js\",\"/_next/static/chunks/00e9ab075e5969cf.js\"],\"\"]\n13:I[33911,[\"/_next/static/chunks/744355e03808d4c7.js\",\"/_next/static/chunks/e4113e168c54d001.js\",\"/_next/static/chunks/bb731efc33baa73a.js\",\"/_next/static/chunks/00e9ab075e5969cf.js\"],\"default\"]\n1d:I[85437,[\"/_next/static/chunks/744355e03808d4c7.js\",\"/_next/static/chunks/e4113e168c54d001.js\",\"/_next/static/chunks/bb731efc33baa73a.js\",\"/_next/static/chunks/00e9ab075e5969cf.js\"],\"Image\"]\n1e:I[97367,[\"/_next/static/chunks/ff1a16fafef87110.js\",\"/_next/static/chunks/d2be314c3ece3fbe.js\"],\"OutletBoundary\"]\n1f:\"$Sreact.suspense\"\n21:I[97367,[\"/_next/static/chunks/ff1a16fafef87110.js\",\"/_next/static/chunks/d2be314c3ece3fbe.js\"],\"ViewportBoundary\"]\n23:I[97367,[\"/_next/static/chunks/ff1a16fafef87110.js\",\"/_next/static/chunks/d2be314c3ece3fbe.js\"],\"MetadataBoundary\"]\n6:[\"$\",\"p\",null,{\"className\":\"text-gray-600 max-w-2xl mx-auto font-jost motion-safe:animate-fade-slide-up\",\"style\":{\"animationDelay\":\"300ms\"},\"children\":\"As AI agents increasingly interact with each other, the real world, and humans, single-agent safety evaluations are no longer sufficient. We study emergent risks in collective action problems, zero-sum competitions, and public goods games.\"}]\n7:[\"$\",\"div\",null,{\"className\":\"mt-8 flex flex-wrap justify-center gap-4 motion-safe:animate-fade-slide-up\",\"style\":{\"animationDelay\":\"420ms\"},\"children\":[\"$\",\"$L12\",null,{\"href\":\"/research\",\"className\":\"inline-flex items-center px-6 py-3 bg-primary-700 text-white font-medium rounded-lg hover:bg-primary-800 transition-colors active:scale-95\",\"children\":[\"View All Research\",[\"$\",\"svg\",null,{\"className\":\"w-4 h-4 ml-2 flex-shrink-0\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"viewBox\":\"0 0 24 24\",\"children\":[\"$\",\"path\",null,{\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"strokeWidth\":2,\"d\":\"M9 5l7 7-7 7\"}]}]]}]}]\n"])</script><script>self.__next_f.push([1,"8:[\"$\",\"section\",null,{\"className\":\"bg-gray-50 py-16 lg:py-20\",\"children\":[\"$\",\"div\",null,{\"className\":\"max-w-6xl mx-auto px-6\",\"children\":[[\"$\",\"$L13\",null,{\"children\":[[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3 mb-3\",\"children\":[[\"$\",\"div\",null,{\"className\":\"w-10 h-10 rounded-lg bg-primary-700 flex items-center justify-center flex-shrink-0\",\"children\":[\"$\",\"svg\",null,{\"className\":\"w-5 h-5 text-white\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"viewBox\":\"0 0 24 24\",\"children\":[\"$\",\"path\",null,{\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"strokeWidth\":2,\"d\":\"M11.049 2.927c.3-.921 1.603-.921 1.902 0l1.519 4.674a1 1 0 00.95.69h4.915c.969 0 1.371 1.24.588 1.81l-3.976 2.888a1 1 0 00-.363 1.118l1.518 4.674c.3.922-.755 1.688-1.538 1.118l-3.976-2.888a1 1 0 00-1.176 0l-3.976 2.888c-.783.57-1.838-.197-1.538-1.118l1.518-4.674a1 1 0 00-.363-1.118l-3.976-2.888c-.784-.57-.38-1.81.588-1.81h4.914a1 1 0 00.951-.69l1.519-4.674z\"}]}]}],[\"$\",\"h2\",null,{\"className\":\"text-2xl font-bold text-gray-900\",\"children\":\"Research\"}]]}],[\"$\",\"p\",null,{\"className\":\"text-gray-600 mb-8 max-w-2xl\",\"children\":\"Published work and ongoing research agenda on testing cooperation in multi-agent LLM systems.\"}]]}],[\"$\",\"$L13\",null,{\"delay\":0.1,\"children\":[\"$\",\"div\",null,{\"className\":\"rounded-xl bg-white p-4 sm:p-6 border border-gray-100\",\"children\":[\"$\",\"div\",null,{\"className\":\"grid grid-cols-1 sm:grid-cols-2 lg:grid-cols-3 gap-6\",\"children\":[[\"$\",\"article\",\"gt-harmbench\",{\"className\":\"group rounded-lg border border-gray-200 p-5 hover:shadow-sm transition-all hover:-translate-y-0.5 bg-gray-50 flex flex-col\",\"children\":[[\"$\",\"div\",null,{\"className\":\"text-xs uppercase tracking-wide text-gray-500 mb-2\",\"children\":\"Preprint 2026\"}],[\"$\",\"h3\",null,{\"className\":\"text-base font-semibold text-gray-900 group-hover:underline underline-offset-4 leading-snug mb-2\",\"children\":[\"$\",\"a\",null,{\"href\":\"https://arxiv.org/abs/2602.12316\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":\"GT-HarmBench: Benchmarking AI Safety Risks Through the Lens of Game Theory\"}]}],[\"$\",\"p\",null,{\"className\":\"text-sm text-gray-600 line-clamp-3 flex-1\",\"children\":\"When AI agents interact in high-stakes settings, do they cooperate or defect? GT-HarmBench stress-tests 15 frontier LLMs across 2,009 scenarios drawn from the MIT AI Risk Repository, structured around classic game-theoretic dilemmas—Prisoner's Dilemma, Stag Hunt, and Chicken. Models reach socially optimal outcomes in only 62% of cases, with cooperation collapsing to 44% in pure Prisoner's Dilemma settings. We uncover a \\\"game theory anchoring effect\\\": explicitly framing a situation in game-theoretic terms nudges models toward selfish Nash strategies, hurting social welfare. Mechanism design interventions—mediation, contracts, and structured communication—recover 14–18% of lost welfare, pointing toward concrete paths for safer multi-agent AI deployment.\"}],[\"$\",\"p\",null,{\"className\":\"mt-3 text-xs text-gray-500\",\"children\":\"Pepijn Cobben*, Xuanqiang Angelo Huang*, Thao Amelia Pham*, Isabel Dahlgren*, Terry Jingchen Zhang, Zhijing Jin\"}],[\"$\",\"div\",null,{\"className\":\"mt-3 flex flex-wrap gap-2\",\"children\":[[\"$\",\"a\",null,{\"href\":\"https://arxiv.org/abs/2602.12316\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"inline-flex text-xs items-center gap-1 rounded border border-primary-300 text-primary-700 px-2 py-1 hover:bg-primary-50 transition-colors\",\"children\":[\"Read paper \",[\"$\",\"span\",null,{\"aria-hidden\":true,\"children\":\"↗\"}]]}],\"$undefined\"]}],[\"$\",\"div\",null,{\"className\":\"mt-3 flex flex-wrap gap-1.5\",\"children\":[[\"$\",\"span\",\"multi-agent safety\",{\"className\":\"text-xs px-2 py-0.5 rounded-full bg-primary-100 text-primary-700\",\"children\":\"multi-agent safety\"}],[\"$\",\"span\",\"game theory\",{\"className\":\"text-xs px-2 py-0.5 rounded-full bg-primary-100 text-primary-700\",\"children\":\"game theory\"}],\"$L14\",\"$L15\",\"$L16\"]}]]}],\"$L17\",\"$L18\",\"$L19\",\"$L1a\",\"$L1b\",\"$L1c\"]}]}]}]]}]}]\n"])</script><script>self.__next_f.push([1,"9:[\"$\",\"section\",null,{\"className\":\"bg-gray-50 py-16 lg:py-20\",\"children\":[\"$\",\"div\",null,{\"className\":\"max-w-6xl mx-auto px-6\",\"children\":[[\"$\",\"$L13\",null,{\"children\":[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3 mb-8\",\"children\":[[\"$\",\"div\",null,{\"className\":\"w-10 h-10 rounded-lg bg-primary-100 flex items-center justify-center flex-shrink-0\",\"children\":[\"$\",\"svg\",null,{\"className\":\"w-5 h-5 text-primary-600\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"viewBox\":\"0 0 24 24\",\"children\":[\"$\",\"path\",null,{\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"strokeWidth\":2,\"d\":\"M3 8l7.89 5.26a2 2 0 002.22 0L21 8M5 19h14a2 2 0 002-2V7a2 2 0 00-2-2H5a2 2 0 00-2 2v10a2 2 0 002 2z\"}]}]}],[\"$\",\"h2\",null,{\"className\":\"text-2xl font-bold text-gray-900\",\"children\":\"Media Contact\"}]]}]}],[\"$\",\"$L13\",null,{\"delay\":0.1,\"children\":[\"$\",\"div\",null,{\"className\":\"grid sm:grid-cols-2 gap-6 mb-8\",\"children\":[[\"$\",\"div\",null,{\"className\":\"bg-white rounded-xl border border-gray-100 shadow-sm p-6\",\"children\":[\"$\",\"div\",null,{\"className\":\"flex items-center gap-4\",\"children\":[[\"$\",\"a\",null,{\"href\":\"https://zhijing-jin.com/home/\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"shrink-0\",\"children\":[\"$\",\"$L1d\",null,{\"src\":\"/images/team/zhijing-jin.png\",\"alt\":\"Zhijing Jin\",\"width\":48,\"height\":48,\"className\":\"w-12 h-12 rounded-full object-cover ring-1 ring-gray-200\"}]}],[\"$\",\"div\",null,{\"children\":[[\"$\",\"a\",null,{\"href\":\"https://zhijing-jin.com/home/\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"font-semibold text-gray-900 hover:text-primary-600 transition-colors\",\"children\":\"Zhijing Jin\"}],[\"$\",\"p\",null,{\"className\":\"text-sm text-gray-500\",\"children\":\"Founder \u0026 Head, Jinesis AI Lab\"}],[\"$\",\"a\",null,{\"href\":\"mailto:zjin.admin@cs.toronto.edu\",\"className\":\"text-sm text-primary-600 hover:underline\",\"children\":\"zjin.admin@cs.toronto.edu\"}]]}]]}]}],[\"$\",\"div\",null,{\"className\":\"bg-white rounded-xl border border-gray-100 shadow-sm p-6\",\"children\":[\"$\",\"div\",null,{\"className\":\"flex items-center gap-4\",\"children\":[[\"$\",\"a\",null,{\"href\":\"https://vesaterra.github.io/\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"shrink-0\",\"children\":[\"$\",\"$L1d\",null,{\"src\":\"/images/team/punya-pandey.png\",\"alt\":\"Punya Syon Pandey\",\"width\":48,\"height\":48,\"className\":\"w-12 h-12 rounded-full object-cover ring-1 ring-gray-200\"}]}],[\"$\",\"div\",null,{\"children\":[[\"$\",\"a\",null,{\"href\":\"https://vesaterra.github.io/\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"font-semibold text-gray-900 hover:text-primary-600 transition-colors\",\"children\":\"Punya Syon Pandey\"}],[\"$\",\"p\",null,{\"className\":\"text-sm text-gray-500\",\"children\":\"Lab Assistant\"}],[\"$\",\"a\",null,{\"href\":\"mailto:ppandey@cs.toronto.edu\",\"className\":\"text-sm text-primary-600 hover:underline\",\"children\":\"ppandey@cs.toronto.edu\"}]]}]]}]}]]}]}],[\"$\",\"$L13\",null,{\"delay\":0.2,\"children\":[\"$\",\"div\",null,{\"className\":\"flex flex-wrap items-center gap-4 text-sm\",\"children\":[[\"$\",\"a\",null,{\"href\":\"https://bsky.app/profile/zhijingjin.bsky.social\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"inline-flex items-center gap-2 px-4 py-2 rounded-lg border border-gray-200 hover:border-primary-200 hover:bg-primary-50 transition-colors\",\"children\":\"Bluesky\"}],[\"$\",\"a\",null,{\"href\":\"https://x.com/ZhijingJin\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"inline-flex items-center gap-2 px-4 py-2 rounded-lg border border-gray-200 hover:border-primary-200 hover:bg-primary-50 transition-colors\",\"children\":\"X / Twitter\"}],[\"$\",\"a\",null,{\"href\":\"https://youtube.com/@Zhijing\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"inline-flex items-center gap-2 px-4 py-2 rounded-lg border border-gray-200 hover:border-primary-200 hover:bg-primary-50 transition-colors\",\"children\":\"YouTube\"}]]}]}]]}]}]\n"])</script><script>self.__next_f.push([1,"a:[\"$\",\"$L13\",null,{\"children\":[\"$\",\"section\",null,{\"className\":\"bg-primary-700 py-16\",\"children\":[\"$\",\"div\",null,{\"className\":\"max-w-4xl mx-auto px-6 text-center\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"text-3xl font-bold text-white mb-4\",\"children\":\"Explore Our Research\"}],[\"$\",\"p\",null,{\"className\":\"text-lg text-primary-100 mb-8 max-w-2xl mx-auto font-jost\",\"children\":\"View all our publications across AI safety, multi-agent systems, and democracy defense.\"}],[\"$\",\"div\",null,{\"className\":\"flex flex-wrap justify-center gap-4\",\"children\":[[\"$\",\"$L12\",null,{\"href\":\"/research\",\"className\":\"inline-flex items-center px-6 py-3 bg-white text-primary-700 font-medium rounded-lg hover:bg-primary-50 transition-colors active:scale-95\",\"children\":[\"All Research\",[\"$\",\"svg\",null,{\"className\":\"w-4 h-4 ml-2 flex-shrink-0\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"viewBox\":\"0 0 24 24\",\"children\":[\"$\",\"path\",null,{\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"strokeWidth\":2,\"d\":\"M9 5l7 7-7 7\"}]}]]}],[\"$\",\"a\",null,{\"href\":\"mailto:eurosafeai.zurich@gmail.com\",\"className\":\"inline-flex items-center px-6 py-3 border-2 border-white text-white font-medium rounded-lg hover:bg-white/10 transition-colors\",\"children\":\"Contact Us\"}]]}]]}]}]}]\n"])</script><script>self.__next_f.push([1,"b:[\"$\",\"footer\",null,{\"className\":\"bg-gray-900 text-white\",\"children\":[[\"$\",\"div\",null,{\"className\":\"max-w-6xl mx-auto px-6 py-12\",\"children\":[\"$\",\"div\",null,{\"className\":\"grid md:grid-cols-4 gap-8\",\"children\":[[\"$\",\"div\",null,{\"className\":\"md:col-span-2\",\"children\":[[\"$\",\"$L1d\",null,{\"src\":\"/images/logo.png\",\"alt\":\"EuroSafeAI\",\"width\":120,\"height\":50,\"className\":\"h-8 w-auto object-contain brightness-0 invert mb-4\"}],[\"$\",\"p\",null,{\"className\":\"text-gray-400 text-sm leading-relaxed mb-4 max-w-sm\",\"children\":\"EuroSafeAI is a nonprofit research organization registered under Swiss law, dedicated to advancing AI safety and security. We have a deep collaboration with the University of Toronto.\"}]]}],[\"$\",\"div\",null,{\"children\":[[\"$\",\"h4\",null,{\"className\":\"font-semibold text-white mb-4\",\"children\":\"Quick Links\"}],[\"$\",\"ul\",null,{\"className\":\"space-y-2\",\"children\":[[\"$\",\"li\",null,{\"children\":[\"$\",\"$L12\",null,{\"href\":\"/\",\"className\":\"text-gray-400 hover:text-white text-sm transition-colors\",\"children\":\"Home\"}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"$L12\",null,{\"href\":\"/research\",\"className\":\"text-gray-400 hover:text-white text-sm transition-colors\",\"children\":\"Research\"}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"$L12\",null,{\"href\":\"/team\",\"className\":\"text-gray-400 hover:text-white text-sm transition-colors\",\"children\":\"Our Team\"}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"$L12\",null,{\"href\":\"/careers\",\"className\":\"text-gray-400 hover:text-white text-sm transition-colors\",\"children\":\"Careers\"}]}]]}]]}],[\"$\",\"div\",null,{\"children\":[[\"$\",\"h4\",null,{\"className\":\"font-semibold text-white mb-4\",\"children\":\"Contact\"}],[\"$\",\"ul\",null,{\"className\":\"space-y-2\",\"children\":[[\"$\",\"li\",null,{\"children\":[\"$\",\"a\",null,{\"href\":\"mailto:eurosafeai.zurich@gmail.com\",\"className\":\"text-gray-400 hover:text-white text-sm transition-colors flex items-center gap-2\",\"children\":[[\"$\",\"svg\",null,{\"className\":\"w-4 h-4 flex-shrink-0\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"viewBox\":\"0 0 24 24\",\"children\":[\"$\",\"path\",null,{\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"strokeWidth\":2,\"d\":\"M3 8l7.89 5.26a2 2 0 002.22 0L21 8M5 19h14a2 2 0 002-2V7a2 2 0 00-2-2H5a2 2 0 00-2 2v10a2 2 0 002 2z\"}]}],[\"$\",\"span\",null,{\"children\":\"eurosafeai.zurich@gmail.com\"}]]}]}],[\"$\",\"li\",null,{\"className\":\"text-gray-400 text-sm flex items-center gap-2\",\"children\":[[\"$\",\"svg\",null,{\"className\":\"w-4 h-4 flex-shrink-0\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"viewBox\":\"0 0 24 24\",\"children\":[[\"$\",\"path\",null,{\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"strokeWidth\":2,\"d\":\"M17.657 16.657L13.414 20.9a1.998 1.998 0 01-2.827 0l-4.244-4.243a8 8 0 1111.314 0z\"}],[\"$\",\"path\",null,{\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"strokeWidth\":2,\"d\":\"M15 11a3 3 0 11-6 0 3 3 0 016 0z\"}]]}],[\"$\",\"span\",null,{\"children\":\"Zurich, Switzerland\"}]]}]]}]]}]]}]}],[\"$\",\"div\",null,{\"className\":\"border-t border-gray-800\",\"children\":[\"$\",\"div\",null,{\"className\":\"max-w-6xl mx-auto px-6 py-4 flex flex-col md:flex-row justify-between items-center gap-2\",\"children\":[[\"$\",\"p\",null,{\"className\":\"text-gray-500 text-sm\",\"suppressHydrationWarning\":true,\"children\":[\"© \",2026,\" EuroSafeAI. All rights reserved.\"]}],[\"$\",\"p\",null,{\"className\":\"text-gray-500 text-sm\",\"children\":\"Swiss nonprofit registered under Swiss Law\"}]]}]}]]}]\n"])</script><script>self.__next_f.push([1,"c:[\"$\",\"script\",\"script-0\",{\"src\":\"/_next/static/chunks/e4113e168c54d001.js\",\"async\":true,\"nonce\":\"$undefined\"}]\nd:[\"$\",\"script\",\"script-1\",{\"src\":\"/_next/static/chunks/bb731efc33baa73a.js\",\"async\":true,\"nonce\":\"$undefined\"}]\ne:[\"$\",\"script\",\"script-2\",{\"src\":\"/_next/static/chunks/00e9ab075e5969cf.js\",\"async\":true,\"nonce\":\"$undefined\"}]\nf:[\"$\",\"$L1e\",null,{\"children\":[\"$\",\"$1f\",null,{\"name\":\"Next.MetadataOutlet\",\"children\":\"$@20\"}]}]\n10:[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$L21\",null,{\"children\":\"$L22\"}],[\"$\",\"div\",null,{\"hidden\":true,\"children\":[\"$\",\"$L23\",null,{\"children\":[\"$\",\"$1f\",null,{\"name\":\"Next.Metadata\",\"children\":\"$L24\"}]}]}],null]}]\n"])</script><script>self.__next_f.push([1,"14:[\"$\",\"span\",\"benchmarking\",{\"className\":\"text-xs px-2 py-0.5 rounded-full bg-primary-100 text-primary-700\",\"children\":\"benchmarking\"}]\n15:[\"$\",\"span\",\"LLM cooperation\",{\"className\":\"text-xs px-2 py-0.5 rounded-full bg-primary-100 text-primary-700\",\"children\":\"LLM cooperation\"}]\n16:[\"$\",\"span\",\"mechanism design\",{\"className\":\"text-xs px-2 py-0.5 rounded-full bg-primary-100 text-primary-700\",\"children\":\"mechanism design\"}]\n"])</script><script>self.__next_f.push([1,"17:[\"$\",\"article\",\"cooperate-or-collapse\",{\"className\":\"group rounded-lg border border-gray-200 p-5 hover:shadow-sm transition-all hover:-translate-y-0.5 bg-gray-50 flex flex-col\",\"children\":[[\"$\",\"div\",null,{\"className\":\"text-xs uppercase tracking-wide text-gray-500 mb-2\",\"children\":\"NeurIPS 2024\"}],[\"$\",\"h3\",null,{\"className\":\"text-base font-semibold text-gray-900 group-hover:underline underline-offset-4 leading-snug mb-2\",\"children\":[\"$\",\"a\",null,{\"href\":\"https://arxiv.org/abs/2404.16698\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":\"Cooperate or Collapse: Emergence of Sustainable Cooperation in a Society of LLM Agents\"}]}],[\"$\",\"p\",null,{\"className\":\"text-sm text-gray-600 line-clamp-3 flex-1\",\"children\":\"We introduce GovSim, a generative simulation platform to study strategic interactions and cooperative decision-making in LLMs facing a Tragedy of the Commons. Agents play as villagers sharing a finite resource across monthly rounds of acting, discussing, and reflecting. Most models fail to achieve sustainable equilibrium (\u003c 54% survival rate); agents leveraging moral reasoning achieve significantly better sustainability.\"}],[\"$\",\"p\",null,{\"className\":\"mt-3 text-xs text-gray-500\",\"children\":\"Giorgio Piatti*, Zhijing Jin*, Max Kleiman-Weiner*, Bernhard Schölkopf, Mrinmaya Sachan, Rada Mihalcea\"}],[\"$\",\"div\",null,{\"className\":\"mt-3 flex flex-wrap gap-2\",\"children\":[[\"$\",\"a\",null,{\"href\":\"https://arxiv.org/abs/2404.16698\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"inline-flex text-xs items-center gap-1 rounded border border-primary-300 text-primary-700 px-2 py-1 hover:bg-primary-50 transition-colors\",\"children\":[\"Read paper \",[\"$\",\"span\",null,{\"aria-hidden\":true,\"children\":\"↗\"}]]}],[\"$\",\"$L12\",null,{\"href\":\"/blog/cooperate-or-collapse\",\"className\":\"inline-flex text-xs items-center gap-1 rounded border border-gray-200 text-gray-600 px-2 py-1 hover:bg-white transition-colors\",\"children\":\"Blog post\"}]]}],[\"$\",\"div\",null,{\"className\":\"mt-3 flex flex-wrap gap-1.5\",\"children\":[[\"$\",\"span\",\"multi-agent LLMs\",{\"className\":\"text-xs px-2 py-0.5 rounded-full bg-primary-100 text-primary-700\",\"children\":\"multi-agent LLMs\"}],[\"$\",\"span\",\"social dilemma\",{\"className\":\"text-xs px-2 py-0.5 rounded-full bg-primary-100 text-primary-700\",\"children\":\"social dilemma\"}],[\"$\",\"span\",\"cooperation\",{\"className\":\"text-xs px-2 py-0.5 rounded-full bg-primary-100 text-primary-700\",\"children\":\"cooperation\"}],[\"$\",\"span\",\"tragedy of the commons\",{\"className\":\"text-xs px-2 py-0.5 rounded-full bg-primary-100 text-primary-700\",\"children\":\"tragedy of the commons\"}],[\"$\",\"span\",\"GovSim\",{\"className\":\"text-xs px-2 py-0.5 rounded-full bg-primary-100 text-primary-700\",\"children\":\"GovSim\"}]]}]]}]\n"])</script><script>self.__next_f.push([1,"18:[\"$\",\"article\",\"moralsim\",{\"className\":\"group rounded-lg border border-gray-200 p-5 hover:shadow-sm transition-all hover:-translate-y-0.5 bg-gray-50 flex flex-col\",\"children\":[[\"$\",\"div\",null,{\"className\":\"text-xs uppercase tracking-wide text-gray-500 mb-2\",\"children\":\"ICLR 2026\"}],[\"$\",\"h3\",null,{\"className\":\"text-base font-semibold text-gray-900 group-hover:underline underline-offset-4 leading-snug mb-2\",\"children\":[\"$\",\"a\",null,{\"href\":\"https://openreview.net/forum?id=XeZ5WBIRvz\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":\"When Ethics and Payoffs Diverge: LLM Agents in Morally Charged Social Dilemmas\"}]}],[\"$\",\"p\",null,{\"className\":\"text-sm text-gray-600 line-clamp-3 flex-1\",\"children\":\"We introduce MoralSim, a framework that tests how large language models navigate situations where ethical principles conflict with financial incentives. Using prisoner's dilemma and public goods games with moral contexts, we evaluated nine frontier models and find that no model exhibits consistently moral behavior. Game structure, moral framing, survival risk, and opponent behavior all significantly influence LLM decision-making.\"}],[\"$\",\"p\",null,{\"className\":\"mt-3 text-xs text-gray-500\",\"children\":\"Steffen Backmann, David Guzman Piedrahita, Emanuel Tewolde, Rada Mihalcea, Bernhard Schölkopf, Zhijing Jin\"}],[\"$\",\"div\",null,{\"className\":\"mt-3 flex flex-wrap gap-2\",\"children\":[[\"$\",\"a\",null,{\"href\":\"https://openreview.net/forum?id=XeZ5WBIRvz\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"inline-flex text-xs items-center gap-1 rounded border border-primary-300 text-primary-700 px-2 py-1 hover:bg-primary-50 transition-colors\",\"children\":[\"Read paper \",[\"$\",\"span\",null,{\"aria-hidden\":true,\"children\":\"↗\"}]]}],\"$undefined\"]}],[\"$\",\"div\",null,{\"className\":\"mt-3 flex flex-wrap gap-1.5\",\"children\":[[\"$\",\"span\",\"moral reasoning\",{\"className\":\"text-xs px-2 py-0.5 rounded-full bg-primary-100 text-primary-700\",\"children\":\"moral reasoning\"}],[\"$\",\"span\",\"social dilemmas\",{\"className\":\"text-xs px-2 py-0.5 rounded-full bg-primary-100 text-primary-700\",\"children\":\"social dilemmas\"}],[\"$\",\"span\",\"multi-agent\",{\"className\":\"text-xs px-2 py-0.5 rounded-full bg-primary-100 text-primary-700\",\"children\":\"multi-agent\"}],[\"$\",\"span\",\"payoff tradeoff\",{\"className\":\"text-xs px-2 py-0.5 rounded-full bg-primary-100 text-primary-700\",\"children\":\"payoff tradeoff\"}],[\"$\",\"span\",\"AI ethics\",{\"className\":\"text-xs px-2 py-0.5 rounded-full bg-primary-100 text-primary-700\",\"children\":\"AI ethics\"}]]}]]}]\n"])</script><script>self.__next_f.push([1,"19:[\"$\",\"article\",\"sanctsim\",{\"className\":\"group rounded-lg border border-gray-200 p-5 hover:shadow-sm transition-all hover:-translate-y-0.5 bg-gray-50 flex flex-col\",\"children\":[[\"$\",\"div\",null,{\"className\":\"text-xs uppercase tracking-wide text-gray-500 mb-2\",\"children\":\"COLM 2025\"}],[\"$\",\"h3\",null,{\"className\":\"text-base font-semibold text-gray-900 group-hover:underline underline-offset-4 leading-snug mb-2\",\"children\":[\"$\",\"a\",null,{\"href\":\"https://arxiv.org/abs/2506.23276\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":\"Corrupted by Reasoning: Reasoning Language Models Become Free-Riders in Public Goods Games\"}]}],[\"$\",\"p\",null,{\"className\":\"text-sm text-gray-600 line-clamp-3 flex-1\",\"children\":\"We examine how language models handle cooperation in multi-agent systems by adapting a public goods game framework. We find that advanced reasoning models like o1 paradoxically underperform at maintaining cooperation compared to traditional LLMs, suggesting that the current approach to improving LLMs—focusing on reasoning capabilities—does not necessarily lead to cooperation. This has important implications for deploying autonomous AI agents in collaborative environments.\"}],[\"$\",\"p\",null,{\"className\":\"mt-3 text-xs text-gray-500\",\"children\":\"David Guzman Piedrahita, Yongjin Yang, Mrinmaya Sachan, Giorgia Ramponi, Bernhard Schölkopf, Zhijing Jin\"}],[\"$\",\"div\",null,{\"className\":\"mt-3 flex flex-wrap gap-2\",\"children\":[[\"$\",\"a\",null,{\"href\":\"https://arxiv.org/abs/2506.23276\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"inline-flex text-xs items-center gap-1 rounded border border-primary-300 text-primary-700 px-2 py-1 hover:bg-primary-50 transition-colors\",\"children\":[\"Read paper \",[\"$\",\"span\",null,{\"aria-hidden\":true,\"children\":\"↗\"}]]}],\"$undefined\"]}],[\"$\",\"div\",null,{\"className\":\"mt-3 flex flex-wrap gap-1.5\",\"children\":[[\"$\",\"span\",\"sanctioning\",{\"className\":\"text-xs px-2 py-0.5 rounded-full bg-primary-100 text-primary-700\",\"children\":\"sanctioning\"}],[\"$\",\"span\",\"public goods\",{\"className\":\"text-xs px-2 py-0.5 rounded-full bg-primary-100 text-primary-700\",\"children\":\"public goods\"}],[\"$\",\"span\",\"reasoning models\",{\"className\":\"text-xs px-2 py-0.5 rounded-full bg-primary-100 text-primary-700\",\"children\":\"reasoning models\"}],[\"$\",\"span\",\"cooperation\",{\"className\":\"text-xs px-2 py-0.5 rounded-full bg-primary-100 text-primary-700\",\"children\":\"cooperation\"}],[\"$\",\"span\",\"free-rider problem\",{\"className\":\"text-xs px-2 py-0.5 rounded-full bg-primary-100 text-primary-700\",\"children\":\"free-rider problem\"}]]}]]}]\n"])</script><script>self.__next_f.push([1,"1a:[\"$\",\"article\",\"agent-to-agent-theory-of-mind\",{\"className\":\"group rounded-lg border border-gray-200 p-5 hover:shadow-sm transition-all hover:-translate-y-0.5 bg-gray-50 flex flex-col\",\"children\":[[\"$\",\"div\",null,{\"className\":\"text-xs uppercase tracking-wide text-gray-500 mb-2\",\"children\":\"Preprint 2025\"}],[\"$\",\"h3\",null,{\"className\":\"text-base font-semibold text-gray-900 group-hover:underline underline-offset-4 leading-snug mb-2\",\"children\":[\"$\",\"a\",null,{\"href\":\"https://arxiv.org/abs/2506.22957\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":\"Agent-to-Agent Theory of Mind: Testing Interlocutor Awareness among Large Language Models\"}]}],[\"$\",\"p\",null,{\"className\":\"text-sm text-gray-600 line-clamp-3 flex-1\",\"children\":\"We investigate how LLMs recognize and adapt to their conversation partners' characteristics, introducing \\\"interlocutor awareness\\\"—an LLM's capacity to identify dialogue partner traits across reasoning patterns, linguistic style, and alignment preferences. LLMs can reliably identify same-family peers and prominent model families like GPT and Claude. This capability enables enhanced multi-agent collaboration but also introduces new vulnerabilities including reward-hacking behaviors and increased jailbreak susceptibility.\"}],[\"$\",\"p\",null,{\"className\":\"mt-3 text-xs text-gray-500\",\"children\":\"Younwoo Choi, Changling Li, Yongjin Yang, Zhijing Jin\"}],[\"$\",\"div\",null,{\"className\":\"mt-3 flex flex-wrap gap-2\",\"children\":[[\"$\",\"a\",null,{\"href\":\"https://arxiv.org/abs/2506.22957\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"inline-flex text-xs items-center gap-1 rounded border border-primary-300 text-primary-700 px-2 py-1 hover:bg-primary-50 transition-colors\",\"children\":[\"Read paper \",[\"$\",\"span\",null,{\"aria-hidden\":true,\"children\":\"↗\"}]]}],\"$undefined\"]}],[\"$\",\"div\",null,{\"className\":\"mt-3 flex flex-wrap gap-1.5\",\"children\":[[\"$\",\"span\",\"theory of mind\",{\"className\":\"text-xs px-2 py-0.5 rounded-full bg-primary-100 text-primary-700\",\"children\":\"theory of mind\"}],[\"$\",\"span\",\"interlocutor awareness\",{\"className\":\"text-xs px-2 py-0.5 rounded-full bg-primary-100 text-primary-700\",\"children\":\"interlocutor awareness\"}],[\"$\",\"span\",\"multi-agent\",{\"className\":\"text-xs px-2 py-0.5 rounded-full bg-primary-100 text-primary-700\",\"children\":\"multi-agent\"}],[\"$\",\"span\",\"adaptation\",{\"className\":\"text-xs px-2 py-0.5 rounded-full bg-primary-100 text-primary-700\",\"children\":\"adaptation\"}],[\"$\",\"span\",\"jailbreak\",{\"className\":\"text-xs px-2 py-0.5 rounded-full bg-primary-100 text-primary-700\",\"children\":\"jailbreak\"}]]}]]}]\n"])</script><script>self.__next_f.push([1,"1b:[\"$\",\"article\",\"govsim-elect\",{\"className\":\"group rounded-lg border border-gray-200 p-5 hover:shadow-sm transition-all hover:-translate-y-0.5 bg-gray-50 flex flex-col\",\"children\":[[\"$\",\"div\",null,{\"className\":\"inline-flex items-center gap-1 rounded border border-amber-400/50 text-amber-700 px-2 py-0.5 text-[11px] self-start mb-2\",\"children\":\"Coming soon\"}],[\"$\",\"h3\",null,{\"className\":\"text-base font-semibold text-gray-900 group-hover:underline underline-offset-4 leading-snug mb-2\",\"children\":\"GovSim-Elect / AgentElect\"}],[\"$\",\"p\",null,{\"className\":\"text-sm text-gray-600 line-clamp-3 flex-1\",\"children\":\"A simulation of elections in multi-agent LLM societies. Examining how AI agents vote, campaign, and coordinate under democratic voting systems—and what incentives shape their electoral behavior.\"}],false,[\"$\",\"div\",null,{\"className\":\"mt-3 flex flex-wrap gap-2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"inline-flex items-center gap-1 rounded border border-red-400/40 text-red-600 px-2 py-1 text-[11px]\",\"children\":\"Paper coming soon\"}],\"$undefined\"]}],[\"$\",\"div\",null,{\"className\":\"mt-3 flex flex-wrap gap-1.5\",\"children\":[[\"$\",\"span\",\"elections\",{\"className\":\"text-xs px-2 py-0.5 rounded-full bg-primary-100 text-primary-700\",\"children\":\"elections\"}],[\"$\",\"span\",\"multi-agent LLMs\",{\"className\":\"text-xs px-2 py-0.5 rounded-full bg-primary-100 text-primary-700\",\"children\":\"multi-agent LLMs\"}],[\"$\",\"span\",\"governance\",{\"className\":\"text-xs px-2 py-0.5 rounded-full bg-primary-100 text-primary-700\",\"children\":\"governance\"}],[\"$\",\"span\",\"simulation\",{\"className\":\"text-xs px-2 py-0.5 rounded-full bg-primary-100 text-primary-700\",\"children\":\"simulation\"}],[\"$\",\"span\",\"democracy\",{\"className\":\"text-xs px-2 py-0.5 rounded-full bg-primary-100 text-primary-700\",\"children\":\"democracy\"}]]}]]}]\n"])</script><script>self.__next_f.push([1,"1c:[\"$\",\"article\",\"coopeval\",{\"className\":\"group rounded-lg border border-gray-200 p-5 hover:shadow-sm transition-all hover:-translate-y-0.5 bg-gray-50 flex flex-col\",\"children\":[[\"$\",\"div\",null,{\"className\":\"inline-flex items-center gap-1 rounded border border-amber-400/50 text-amber-700 px-2 py-0.5 text-[11px] self-start mb-2\",\"children\":\"Coming soon\"}],[\"$\",\"h3\",null,{\"className\":\"text-base font-semibold text-gray-900 group-hover:underline underline-offset-4 leading-snug mb-2\",\"children\":\"CoopEval\"}],[\"$\",\"p\",null,{\"className\":\"text-sm text-gray-600 line-clamp-3 flex-1\",\"children\":\"Benchmarking cooperation-sustaining mechanisms and LLM agents in social dilemmas. Translating game-theoretic mechanisms to real evaluation settings to identify what makes cooperation robust at scale.\"}],false,[\"$\",\"div\",null,{\"className\":\"mt-3 flex flex-wrap gap-2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"inline-flex items-center gap-1 rounded border border-red-400/40 text-red-600 px-2 py-1 text-[11px]\",\"children\":\"Paper coming soon\"}],\"$undefined\"]}],[\"$\",\"div\",null,{\"className\":\"mt-3 flex flex-wrap gap-1.5\",\"children\":[[\"$\",\"span\",\"cooperation\",{\"className\":\"text-xs px-2 py-0.5 rounded-full bg-primary-100 text-primary-700\",\"children\":\"cooperation\"}],[\"$\",\"span\",\"benchmarking\",{\"className\":\"text-xs px-2 py-0.5 rounded-full bg-primary-100 text-primary-700\",\"children\":\"benchmarking\"}],[\"$\",\"span\",\"social dilemmas\",{\"className\":\"text-xs px-2 py-0.5 rounded-full bg-primary-100 text-primary-700\",\"children\":\"social dilemmas\"}],[\"$\",\"span\",\"mechanism design\",{\"className\":\"text-xs px-2 py-0.5 rounded-full bg-primary-100 text-primary-700\",\"children\":\"mechanism design\"}]]}]]}]\n"])</script><script>self.__next_f.push([1,"22:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n"])</script><script>self.__next_f.push([1,"25:I[27201,[\"/_next/static/chunks/ff1a16fafef87110.js\",\"/_next/static/chunks/d2be314c3ece3fbe.js\"],\"IconMark\"]\n20:null\n"])</script><script>self.__next_f.push([1,"24:[[\"$\",\"title\",\"0\",{\"children\":\"Multi-Agent Safety | EuroSafeAI\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"Research on testing LLM cooperation and safety in multi-agent simulation settings, including game-theoretic benchmarking and social dilemma experiments.\"}],[\"$\",\"meta\",\"2\",{\"name\":\"author\",\"content\":\"EuroSafeAI\"}],[\"$\",\"meta\",\"3\",{\"name\":\"keywords\",\"content\":\"AI Safety,AI Security,Multi-Agent Safety,Research,Nonprofit,Switzerland\"}],[\"$\",\"meta\",\"4\",{\"property\":\"og:title\",\"content\":\"EuroSafeAI - Developing Multi-Agent AI Safety for Democracy\"}],[\"$\",\"meta\",\"5\",{\"property\":\"og:description\",\"content\":\"A nonprofit research organization led by Prof. Zhijing Jin, aiming to advance AI safety and security through rigorous research, threat assessment, and mitigation strategies.\"}],[\"$\",\"meta\",\"6\",{\"property\":\"og:locale\",\"content\":\"en_US\"}],[\"$\",\"meta\",\"7\",{\"property\":\"og:type\",\"content\":\"website\"}],[\"$\",\"meta\",\"8\",{\"name\":\"twitter:card\",\"content\":\"summary_large_image\"}],[\"$\",\"meta\",\"9\",{\"name\":\"twitter:title\",\"content\":\"EuroSafeAI - Developing Multi-Agent AI Safety for Democracy\"}],[\"$\",\"meta\",\"10\",{\"name\":\"twitter:description\",\"content\":\"A nonprofit research organization led by Prof. Zhijing Jin, aiming to advance AI safety and security through rigorous research, threat assessment, and mitigation strategies.\"}],[\"$\",\"link\",\"11\",{\"rel\":\"icon\",\"href\":\"/favicon.ico\",\"sizes\":\"any\"}],[\"$\",\"link\",\"12\",{\"rel\":\"apple-touch-icon\",\"href\":\"/favicon.ico\"}],[\"$\",\"$L25\",\"13\",{}]]\n"])</script></body></html>